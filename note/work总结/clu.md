- 序列标注
	- ner_span
		- 半指针半标注的意思是，通过两个线性层分别预测序列中实体span的起始位置和结束位置，并且每个position的输出不是0，1二分类，而是多分类，表示是不同类型的span起始位置，或者结束位置。
		- $[B,S]->[B,S,H]->[B,S,N+1]$   N表示标签类别数量，加上'O'类型
		- 输出两个$[B,S,N]$序列，一个用于指示所有span的起始位置，一个用于指示所有span的结束位置。
		- 相当于对每个token做多分类任务
		- 解码预测时候每个token位置取最大概率的类别作为预测的span类型`torch.argmax(start_logits, -1)` $[B,S,N+1]->[B,S]$，参考脚本task_sequence_labeling_ner_span.py 非常清楚
	- ner_mrc
		- 把不同的实体类型做多次推理计算，每次识别一种类型的实体的span起始位置和结束位置
		- start_logits: $[B,S,H]->[B,S,2]$
		- end_logits: $[B,S,H]->[B,S,2]$
		- 2表示用于预测属于实体和非实体的概率，这样应该是为了和多分类的形式一样，解码方便

	- ner_global_pointer
	- 序列标注一个核心的问题是标签不平衡问题
		- 权重平和
		- 样本重要程度 FocalLoss
	- 
	- 用mask去除padding部分参与loss计算
- prompt learning
	- PET
		- 在句子前面拼接前缀："[mask]满意"
		- 如果为正例，label对应mask的位置为"很"
		- 如果为负例，label对应mask的位置为"不"
		- 整个任务就是mask_lm，预测mask位置为"很"和"不"的概率大小
		- 可以结合mlm预训练在一起做
	- p-tuning
		- 只在embedding层添加连续prompt，冻结网络中其他层，只优化这些虚拟连续token
		- 为什么有效：
			- 不管是PET还是P-tuning，它们其实都更接近预训练任务，而加个全连接层的做法，其实还没那么接近预训练任务，所以某种程度上来说，P-tuning有效更加“显然”，反而是加个全连接层微调为什么会有效才是值得疑问的。

	- prefix-tuning
		- 网络每一层都添加虚拟token，只优化这些token，冻结网络其他参数
		- 怎么做到每一层都有：
			- 每一层的prefix部分是从对应的embedding表中复制过来的，在每一层，后面token计算的时候都会attention这些虚拟token



- 交叉熵
	- 核心公式 $-sum(p*log(q))$

- 对比学习，文本表征，双塔模型

- 零样本启动
    - 基于标签语义
    - 基于小样本或零样本的提示学习
	    - 魔搭


- 图谱生成
	- AI生成节点
	- 节点关联标签（相似度）
	- 标签查询企业（sql）
	- 改进方案：
		- AI生成节点
		- 节点扩展为句子，编码为查询向量（6B模型）
		- 企业信息编码为向量库
		- 基于向量检索
		- 需要训练一个双塔模型，左边为查询语句，右边为相关企业（对比学习），考虑把CD-TOM模型用在这里
		- 为什么用这个模型，为了对抗噪声
		- 
	- 优点
		- 减少环节，既提高了效率，也减少了多环节中的错误累积

- 对比学习CD-TOM的作用
	- 使用标签去检索相关的企业或新闻
	- 对每个新增的数据去检索相关的标签

- 缺少数据问题
	- mt-DNN 样本比较少的情况下，除了数据增强，多任务学习也有提升
		- https://zhuanlan.zhihu.com/p/56868716