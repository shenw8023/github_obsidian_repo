# BN
- zhangjunlin https://zhuanlan.zhihu.com/p/38176412
	- BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的
	- 为什么深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢？这是个在DL领域很接近本质的好问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，BN本质上也是解释并从某个不同的角度来解决这个问题的。
	- 所谓白化，就是对输入数据分布变换到0均值，单位方差的正态分布
	- 而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其<mark style="background: #FFB8EBA6;">实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域</mark>，这样输入的小变化就会导致损失函数较大的变化，<mark style="background: #FFB8EBA6;">意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</mark>

- nndl
	- 在深度神经网络中，中间某一层的输入是其之前的神经层的输出。因此，其之前的神经层的参数变化会导致其输入的分布发生较大的差异。在使用随机梯度下降来训练网络时，每次参数更新都会导致网络中间每一层的输入的分布发生改变。越深的层，其输入的分布会改变得越明显。就像一栋高楼，低楼层发生一个较小的偏移，都会导致高楼层较大的偏移。

	- 从机器学习角度来看，如果某个神经层的输入分布发生了改变，那么其参数需要重新学习，这种现象叫做内部协变量偏移（Internal Covariate Shift）。
	- 一般的机器学习算法都要求输入在训练集和测试集上的分布是相似的。如果不满足这个假设，在训练集上学习到的模型在测试集上的表现会比较差。
	- 对净输入 z (l) 的标准归一化会使得其取值集中到 0 附近，如果使用 sigmoid型激活函数时，这个取值区间刚好是接近线性变换的区间，减弱了神经网络的非线性性质。因此，为了使得归一化不对网络的表示能力造成负面影响，可以通过一个附加的缩放和平移变换改变取值区间。

- 背景
	- 为什么要拉回到统一的分布
	- 拉回到标准正态分布的好处
	- 存在的问题和修正
- 具体在哪个环节
	- 激活函数前，线性计算后（虽然也可以用在线性计算之前，但其分布性质不如前者稳定）
	- 是要在激活之前做归一化，把输入拉回激活函数梯度大的区域
- 计算方式
	- 是对每个神经元做BN操作的，每个神经元用到的E和V都是不一样的，但都是根据batch数量的该数值得到的
	- 每个神经元独立做BN，互不影响
	- 就相当于对输入向量的每个维度值在batch的维度上做归一化
- 推理阶段：
	- 当训练完成时，用整个数据集上的均值 µ 和方差 σ 来分别代替每次小批量样本的均值和方差 ，在实践中均值和方差也可以用移动平均来计算。
- 带来的好处
	- BN比LN在inference的时候快，因为不需要计算mean和variance，直接用running mean和running variance就行。
- 缺点
	- 在mini-batch较小的情况下不太适用。
		- BN是对整个mini-batch的样本统计均值和方差，当训练样本数很少时，样本的均值和方差不能反映全局的统计分布信息，从而导致效果下降。
	- 无法应用于RNN
		- 对于NLP data来说，batch上去做归一化是没啥意义的，因为不同句子的同一位置的分布大概率是不同的。
	- 



# LN
- https://blog.csdn.net/qq_43827595/article/details/121877901
- 在标准循环神经网络中，循环神经层的净输入一般会随着时间慢慢变大或变小，从而导致梯度爆炸或消失。而层归一化的循环神经网络可以有效地缓解这种状况。
- $\gamma和 \beta$应该是可学习的 


- 一言以蔽之。**BN是对batch的维度去做归一化，也就是针对不同样本的同一特征做操作。LN是对hidden的维度去做归一化，也就是针对单个样本的不同特征做操作。**因此**LN可以不受样本数的限制。**
- 求均值和方差的差异：
	- BN是把除了轴num_features外的所有轴的元素放在一起，取平均值和方差的，然后对每个元素进行归一化，最后再乘以对应的$\gamma$和$\beta$（<mark style="background: #FFB8EBA6;">共享</mark>）。<mark style="background: #FFB8EBA6;">BN共有num_features个mean和var</mark>，（假设输入数据的维度为(N,num_features, H, W））。
	- 而LN是把normalized_shape这几个轴的元素都放在一起，取平均值和方差的，然后对每个元素进行归一化，最后再乘以对应的$\gamma$和$\beta$（每个元素不同）。<mark style="background: #FFB8EBA6;">LN共有N1*N2个mean和var</mark>（假设输入数据的维度为(N1,N2,normalized_shape），normalized_shape表示多个维度）


