- [参考](https://zhuanlan.zhihu.com/p/589054073)

- 场景：
	- 为什么使用prompt
		- 业务场景中不断出现新的实体类型、关系类型、事件类型，无法增量训练模型，
		- 每次来了新业务场景，要不增加新业务相关数据集，重新训练一个模型（不能保证之前任务效果不会受到影响）；重新训练浪费精力和时间
		- 要不就新增一个场景的模型，时间长了以后模型的数量越来越多，部署的资源要求越来越高。
		- 不同的业务场景，实际就是关注的句子中信息不同，能不能通过指定任务要关注的信息，去提取不同类型的结果呢？
		- 想到提示学习，添加离散的prompt指定要关注的主体信息，实现指定场景下的抽取；考虑到离散prompt一般都没有连续prompt效果好，参考p-tuning方法，再结合部分连续prompt，提升效果。
		- <mark style="background: #FFB8EBA6;">更主要的原因是冷启动问题，有了通用模型利用通用模型具备的知识和强大理解能力，就能小样本微调，或零样本学习</mark>。
	- 为什么使用指针网络：
		- 原始的指针网络解决的问题是什么
			- 生成任务中，输出序列词典大小依赖于输入序列元素数量的场景。
			- 在生成阶段的某个step时，计算当前embedding与输入序列的attention，然后直接将该attention结果做softmax，取概率最大的输入token复制过来作为当前时间步的输出
			- 在这里的改造类似，输出的span索引对应输入序列中的span
		- 能解决实体和关系嵌套、重叠问题。
		- 缺点：标签不平衡问题，使用Focal-loss
	

- 训练数据构造，总30w的数据
	- IREE 事件抽取
		- 10w
	- DuEEfin 事件抽取
		- 1w
	- DuEE1.0 事件抽取
		- 1w
	- DuIE2.0 关系抽取
		- 20w
	- 去重和融合，例如同一句子的NER结果做融合
	- 负例很重要
	- 实体识别数据集，直接拿过来用
	- 关系抽取数据集，转换为
		- "谭孝曾是谭元寿的长子，也是谭派第六代传人"
			- prompt："人物"， result: ["谭孝曾", "谭元寿"]
			- prompt: 

- 基本方法：
	- 本质是提示学习 + 指针网络(解决嵌套实体问题)
	- 单次推理，构建为：`[CLS]prompt[SEP]sentence[SEP]`
	- 输出：两个序列，分别对应span的start索引和end索引，shape都为[B,S]，其中元素大于阈值表示该position为span的起始索引或end索引
	- 解码的时候要过滤掉prompt部分

	- NER：
		- schema结构为 `List` 类型，包含所有要提取的 `实体类型`。
		- prompt对应某个实体类型，如`["人物"，"时间"，"地点"]`
		- 输出中，每个实体类型对应多个span
		- NER Results: `{ '人物': ['谭孝曾', '谭元寿'] }`
	- EE：
		- 事件抽取的schema结构为 `Dict` 类型，其中 `Key` 的值是所有 `事件触发词`，`Value` 对应每一个触发词下的所有 `事件属性或角色`。
		- 例如 `{"出行触发词": ['时间', '出发地', '目的地', '花费'],}`
		- 抽取trigger-span：
			- 将trigger作为prompt
			- 每个trigger都会做一次推理
		- 抽取属性-span：
			- <mark style="background: #FF5582A6;">将抽取到的trigger-span + "的"  +  属性词 拼接起来作为prompt，抽取相应的属性-span</mark>
			- 例如：`离开+"的"+"时间"`
			- subject对应的value中的每个元素都会做一次推理
	- SPO：
		- 信息抽取的schema结构为 `Dict` 类型，其中 `Key` 的值是所有 `主语`，`Value` 对应该主语对应的所有 `谓语或属性`。
		-  SPO Results:  `{ '谭孝曾': { '父亲': ['谭元寿'] }, '谭元寿': { '父亲': [] } }`
- loss
	- start 序列和end 序列分别使用BCE-loss，再取平均
	- <mark style="background: #FF5582A6;">优化：Focal-loss缓解正负例不平衡问题</mark>

## 数据增强：
- 场景的增强方式：
	- （1）**修改原有训练数据样本**；缺点：缺乏语义多样性，只能生成语义相似的样本
		- 回译
		- 同义词替换
		- BERT掩码换词
	- （2）**生成+采样**。事件抽取任务需要在保持事件结构(触发器和参数)不变的情况下增加训练数据，因此“生成+采样”的方法并不适用
	
- SwapSPO 策略
	- 将相同P的句子分成一组，随机交换这些句子中的S-O-pair
	- 例如"A公司融资xx金额"， "B企业经过xx操作，获得xx万元的投资"
	- 获得同一关系的不同表述
- 正例：Mask Then Fill 策略介绍
	- 基于生成模型的信息抽取数据增强策略
	- 先定义两种类型文本片段：（1）事件相关片段（触发词和事件要素）；（2）**附加片段**。然后随机掩码一个附件片段，最后采用微调后的T5模型进行文本填充，<mark style="background: #FFB8EBA6;">使用top-p和top-k采样增加结果多样性。</mark>
	- T5生成模型训练样本构造：
		- 随机获取一个文本片段
		- 将原始文本切割成个片段；
		- 随机选择一个文本片段来替换为[MASK]符号。替换的span被用作目标，在填充数据上对T5模型进行微调，得到最终模型。
		- T5模型使用的是追一科技开源的中文T5模型
- 负例：自分析负例生成（Auto Neg）策略介绍
	- 部分P容易出现混淆，误召回现象
	- 基于第一版模型在测试集上预测结果，自动发现易混淆predicate
	- 对易混淆的P做负例生成
		- `'上级行政区': ['地理位置', '所属机构', '行政区等级']`
		- content："A市隶属xx管理"
		- ``{"content": ""A市隶属xx管理", "result_list": [], "prompt": "A市的地理位置"},
		- ``{"content": ""A市隶属xx管理", "result_list": [], "prompt": "A市的所属机构"},
		- ``{"content": ""A市隶属xx管理", "result_list": [], "prompt": "A市的行政区等级"},
- 效果：
	- <mark style="background: #FFB8EBA6;">在小样本上做对比实验，先定性分析，再全量训练</mark>
	- Auto Neg有4个点提升 （主要是纠正错例）
	- Mask Then Fill 提升比swap SPO 明显，有6个点提升（主要是提高了数据丰富度）



## 改进点：
- 添加连续prompt，结合prompt-tuning的方法，提升效果
- 三种数据增强方法
- focal-loss 平衡标签
