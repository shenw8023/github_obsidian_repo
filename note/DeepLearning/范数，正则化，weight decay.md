
- weight decay和正则化联系
	- 在标准的随机梯度下降中，**权重衰减正则化和L2正则化**的效果相同
	- 因此，权重衰减在一些深度学习框架中通过 L2 正则化来实现
	- 但是，在较为复杂的优化方法( 比如Adam ) 中，权重衰减正则化和正则化并不等价
	-  一般设置为`weight_decay` = 1e-4，其对应 ![\lambda](https://private.codecogs.com/gif.latex?%5Cinline%20%5Clambda) 称为**正则化参数**
- 为什么偏置不需要正则化
	- 因为在对输出结果的贡献中，参数b对于输入的改变是不敏感的，不管输入改变是大还是小，参数b的贡献就只是加个偏置而已。
- L1正则化的应用场景：
	- 随着海量数据处理的兴起，工程上对于模型稀疏化的要求也随之出现了。这时候，L2正则化已经不能满足需求，因为它只是使得模型的参数值趋近于0，而不是等于0，这样就无法丢掉模型里的任何一个特征，因此无法做到稀疏化。这时，L1的作用随之显现。L1正则化的作用是使得大部分模型参数的值等于0，这样一来，当模型训练好后，这些权值等于0的特征可以省去，从而达到稀疏化的目的，也节省了存储的空间，因为在计算时，值为0的特征都可以不用存储了。
	- L1正则化对于所有权重予以同样的惩罚，也就是说，不管模型参数的大小，对它们都施加同等力度的惩罚，因此，较小的权重在被惩罚后，就会变成0。因此，在经过L1正则化后，大量模型参数的值变为0或趋近于0，当然也有一部分参数的值飙得很高。由于大量模型参数变为0，这些参数就不会出现在最终的模型中，因此达到了稀疏化的作用，这也说明了<mark style="background: #FFB8EBA6;">L1正则化自带特征选择的功能</mark>，这一点十分有用。

- 范数的定义：
	- 范数用来表征向量空间的大小，[参考](https://blog.csdn.net/qq_36512295/article/details/88824622)
	- ![[Pasted image 20230621141633.png]]
- 范数对应到距离计算
	- 曼哈顿距离
	- 欧几里得距离
		- ![[Pasted image 20230621141608.png]]
	- 闵可夫斯基距离，对应P范数

- L1 和 L2 范数在机器学习上最主要的应用大概分下面**两类**：
	- 作为损失函数使用：<mark style="background: #FFB8EBA6;">利用的是范数对应为距离度量中的公式形式</mark>
	- 作为正则项使用也即所谓 L1-regularization 和 L2-regularization：<mark style="background: #FFB8EBA6;">利用的是范数本身的定义的公式形式</mark>
	- [具体参考](https://blog.csdn.net/qq_44766883/article/details/111416264)<mark style="background: #FFB8EBA6;">说的非常好</mark>
- L1和L2范数作为损失函数
	- L1-norm 损失函数：MAE，平均绝对误差
	- L2-norm 损失函数：MSE，均方误差，最小二乘误差
		- ![[Pasted image 20230621141116.png]]
- 作为正则化项：
	- ![[Pasted image 20230621143721.png]]
- 为什么L1正则能达到稀疏化或特征选择的作用
	- 梯度更新时，不管 L1 的大小是多少（只要不是0）梯度都是1或者-1，所以每次更新时，它都是稳步向0前进。
	- 而看 L2 的话，就会发现它的梯度当w越靠近0，就变得越小。
	- 也就是说加了 L1 正则的话基本上经过一定步数后很可能变为0，而 L2 几乎不可能，因为在值小的时候其梯度也会变小。于是也就造成了 L1 输出稀疏的特性。
- 对比学习中把表征向量投影到单位超球面上，利用L2正则具体做法：
	- ![[Pasted image 20230621145012.png]]
	- 将向量每个元素除以它的L2范数
	- 结果是处理后的向量模长为1
	- 相比带有向量长度信息的点积，在去掉长度信息后的单位长度向量上操作，能增加深度学习模型的训练稳定性