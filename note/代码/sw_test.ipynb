{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_chinese_chars(text):\n",
    "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cp = ord(char)\n",
    "        if _is_chinese_char(cp):\n",
    "            output.append(\" \")\n",
    "            output.append(char)\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            output.append(char)\n",
    "    # return \"\".join(output)\n",
    "    return output\n",
    "def _is_chinese_char(cp):\n",
    "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "    #\n",
    "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "    # space-separated words, so they are not treated specially and handled\n",
    "    # like the all of the other languages.\n",
    "    if (\n",
    "        (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "        or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "        or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "        or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "        or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "        or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "        or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "        or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "    ):  #\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"中国人民attention中过\"\n",
    "res = _tokenize_chinese_chars(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '中', ' ', ' ', '国', ' ', ' ', '人', ' ', ' ', '民', ' ', 'a', 't', 't', 'e', 'n', 't', 'i', 'o', 'n', ' ', '中', ' ', ' ', '过', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中', '过', 'att']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"  中  过  att\"\n",
    "a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _run_split_on_punc(text, never_split=None):\n",
    "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "    if never_split is not None and text in never_split:\n",
    "        return [text]\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "        char = chars[i]\n",
    "        if _is_punctuation(char):\n",
    "            output.append([char])\n",
    "            start_new_word = True\n",
    "        else:\n",
    "            if start_new_word:\n",
    "                output.append([])\n",
    "            start_new_word = False\n",
    "            output[-1].append(char)\n",
    "        i += 1\n",
    "\n",
    "    # return [\"\".join(x) for x in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['d', 'a', 'f'], [','], ['d', 'f', 'a'], ['*'], ['d', 'a', 'f']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_run_split_on_punc(\"daf,dfa*daf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw_run_split_on_punc(text):\n",
    "    output = [[]]\n",
    "    for i in text:\n",
    "        if _is_punctuation(i):\n",
    "            output.append([i])\n",
    "            output.append([])\n",
    "        else:\n",
    "            output[-1].append(i)\n",
    "    return [i for i in output if i!=[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordpieceTokenizer(object):\n",
    "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            \n",
    "            \"\"\"\n",
    "           \n",
    "            外层循环start从0往后\n",
    "                内层end每次从token尾往前\n",
    "                    如果他俩夹的串在vocab中，切出子串\n",
    "                    start增加子串的长度\n",
    "            \n",
    "            注意start为0时，子串前不加\"##\"\n",
    "            否则子串前加\"##\"\n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            while start < len(chars):  # start 0, \n",
    "                end = len(chars)       # end 2\n",
    "                cur_substr = None\n",
    "                while start < end:      \n",
    "                    substr = \"\".join(chars[start:end]) # at\n",
    "                    if start > 0:                      \n",
    "                        substr = \"##\" + substr         # ##at\n",
    "                    if substr in self.vocab:            \n",
    "                        cur_substr = substr            # cur_substar=##at\n",
    "                        break\n",
    "                    end -= 1                           # end=2\n",
    "                if cur_substr is None:     # 如果在某一轮发现了token中有字符不在vocab中，那么整个token视为unk_token\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end             # start跳到切出去的子串位置\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = WordpieceTokenizer([\"at\", \"##ten\", \"##tion\", \"plea\", \"##s\", \"##e\"], \"[unk]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at', '##ten', '##tion', 'plea', '##s', '##e']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpt.tokenize(\"attention please\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\python3.8.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6312,  1.2852, -1.8901]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6312,  1.2852, -1.8901],\n",
       "        [-0.6312,  1.2852, -1.8901],\n",
       "        [-0.6312,  1.2852, -1.8901]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.expand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6312,  1.2852, -1.8901],\n",
       "         [-0.6312,  1.2852, -1.8901],\n",
       "         [-0.6312,  1.2852, -1.8901]],\n",
       "\n",
       "        [[-0.6312,  1.2852, -1.8901],\n",
       "         [-0.6312,  1.2852, -1.8901],\n",
       "         [-0.6312,  1.2852, -1.8901]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.expand(2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1).contiguous().eq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = a.view(-1).contiguous().eq(1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(len(mask))[mask].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = torch.nn.Linear(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[5,4] -(3,4) > [5,3]\n",
    "[5,4] -(2,4) > [5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1996,  0.4603, -0.2883,  0.1713],\n",
       "        [-0.1496,  0.1463, -0.2608,  0.0918],\n",
       "        [-0.2600,  0.1853, -0.0806,  0.2524]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1996,  0.4603, -0.2883,  0.1713],\n",
       "        [-0.1496,  0.1463, -0.2608,  0.0918]], grad_fn=<IndexSelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.index_select(0, torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2172,  0.0892,  0.1601,  0.1991, -0.1320],\n",
       "        [-0.0744,  0.3415, -0.2958, -0.1294,  0.1244],\n",
       "        [-0.3874, -0.2104, -0.1770, -0.2696, -0.2908],\n",
       "        [-0.0500, -0.1520,  0.4151, -0.2474,  0.0696],\n",
       "        [-0.2954,  0.2502, -0.1591, -0.2732,  0.0095]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = torch.nn.Linear(5,5)\n",
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4277, -0.4221,  0.3778, -0.1997,  0.4031], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def prune_linear_layer(layer, index, dim=0):\n",
    "    \"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n",
    "        Return the pruned layer as a new layer with requires_grad=True.\n",
    "        Used to remove heads.   \n",
    "    \"\"\"                                    # layer:(4,5)\n",
    "    index = index.to(layer.weight.device)  # [m,4] W[5,4]     [m,5]\n",
    "    W = layer.weight.index_select(dim, index).clone().detach()\n",
    "    if layer.bias is not None:\n",
    "        if dim == 1:\n",
    "            b = layer.bias.clone().detach()\n",
    "        else:\n",
    "            b = layer.bias[index].clone().detach()\n",
    "    new_size = list(layer.weight.size())  #[5,4]\n",
    "    new_size[dim] = len(index)          #[2,4]\n",
    "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device) #(4,2)\n",
    "    new_layer.weight.requires_grad = False\n",
    "    new_layer.weight.copy_(W.contiguous())\n",
    "    new_layer.weight.requires_grad = True\n",
    "    if layer.bias is not None:\n",
    "        new_layer.bias.requires_grad = False\n",
    "        new_layer.bias.copy_(b.contiguous())\n",
    "        new_layer.bias.requires_grad = True\n",
    "    return new_layer   #(infeature, len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(4,5)\n",
    "index = torch.tensor([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0]\n",
      "2\n",
      "{2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "pruned_heads =set([2,3,4])\n",
    "heads = [4]\n",
    "for head in heads:\n",
    "    a =[1 if h < head else 0 for h in pruned_heads]\n",
    "    print(a)\n",
    "    head = head - sum(1 if h < head else 0 for h in pruned_heads)\n",
    "    print(head)\n",
    "pruned_heads = pruned_heads.union(heads)\n",
    "# print(pruned_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\python3.8.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):  # config.hidden_size, config.num_attention_heads\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads !=0:\n",
    "            raise ValueError(\n",
    "                \"the hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            ) \n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "\n",
    "    def transpose_for_scores(self, x): # x: [B,S,H]\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape) #[B,S,num_heads,head_size]\n",
    "        return x.permute(0, 2, 1, 3) #[B,num_heads,S,head_size]\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        option,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,   \n",
    "    ):  \n",
    "\n",
    "        mixed_query_layer = self.query(hidden_states)  # [B,S,H]\n",
    "        mixed_key_layer = self.key(hidden_states) \n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        \n",
    "        # 拆分为多头\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer) #[B,num_heads, S, head_size]\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # query * key 计算得分矩阵\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1,-2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # 权重矩阵\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # mask heads\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer) # [B,num_heads,S,S], [B,num_heads,S,head_size] -> [B,num_heads,S,head_size]\n",
    "        \n",
    "        # 合并多头\n",
    "        if option == 1:\n",
    "            context_layer = context_layer.permute(0,2,1,3).contiguous() #[B,S,num_heads,head_size]\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "            context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        else:\n",
    "            context_layer = context_layer.permute(0,2,1,3)\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "            context_layer = context_layer.reshape(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 100\n",
    "        self.num_attention_heads = 5\n",
    "        self.attention_probs_dropout_prob = 0\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertSelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input= torch.randn(3,10, 100)\n",
    "res1 = model(input, option=1)\n",
    "res2 = model(input, option=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.equal(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2324, -0.7838, -0.1866, -1.8796],\n",
       "        [-0.1204, -0.4605,  0.3686, -0.6623],\n",
       "        [-0.4158,  0.3718, -1.0046, -2.1423]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2324, -0.7838, -0.1866, -1.8796]]],\n",
       "\n",
       "\n",
       "        [[[-0.1204, -0.4605,  0.3686, -0.6623]]],\n",
       "\n",
       "\n",
       "        [[[-0.4158,  0.3718, -1.0046, -2.1423]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9577, -0.0650,  0.9996])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a826ce577a0a12051b7a8c693e7fd244e31ca35db2956213af29c1d46e9d8551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
