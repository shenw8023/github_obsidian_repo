
- 本质：
	- 数据并行，模型并行，流水线并行，在三种并行方式中，数据并行因其易用性，得到了最为广泛的应用。然而，数据并行会产生大量冗余 Model States 的空间占用。ZeRO 的本质，是在数据并行的基础上，对冗余空间占用进行深度优化。
- 大规模训练中的显存占用可以分为 **Model States** 与 **Activation** 两部分，而 **ZeRO** 就是为了解决 **Model States** 而诞生的一项技术。
- Model States包括：
	- 优化器参数
	- 梯度信息
	- 模型参数

- ZeRO2:
	- 梯度目的是计算参数更新量
	- 如果每张卡仅处理部分参数对应的优化器参数，那么该卡也只涉及部分参数的梯度
	- 完成部分参数的更新后，通过All Gather 分发不同部分参数的更新量
	- 每张卡的部分梯度被计算出后，通过All Reduce聚合获取所有卡上的平均梯度


- 第一，可以发现，模型训练对显存的占用可以分为两部分：一部分是模型 forward 时保存下来的临时变量，这部分显存会在反向传播时会逐渐释放掉，这部分一般被称为 Activations。另一部分则是参数、梯度等状态信息占用的显存，这部分一般被称为 Model States。
- 第二，Forward 结束后的显存占用峰值时刻决定了是否会碰到显存墙。降低其余部分的显存占用没有意义，**关键在于削低峰值**。