- pytorch官方实现的zero #TODO 
	- torch.distributed.fsdp
- Transformers.Trainer包装了torch.fsdp
- DeepSpeed实现zero
- Accelerate 集成了DeepSpeed

- zero的选择：
	- torch api
	- 利用deepspeed
	- 利用Accelerate集成的deepspeed
- pytorch doc FSDP tutorial
	- https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
	- min_num_params， auto_wrap #TODO 
- pytorch FSDP api
	- https://pytorch.org/docs/stable/fsdp.html
- https://huggingface.co/blog/pytorch-fsdp