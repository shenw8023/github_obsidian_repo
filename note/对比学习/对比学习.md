- [参考张俊林](https://zhuanlan.zhihu.com/p/367290573)
- 那对比学习是要干什么呢？从目标来说，对比学习就是要干NLP领域类似Bert预训练的事情，也即是上面那几句话。
- 目前，对比学习貌似处于“无明确定义、有指导原则”的状态，它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。而如何构造相似实例，以及不相似实例，如何构造能够遵循上述指导原则的表示学习模型结构，以及如何防止模型坍塌(Model Collapse)，这几个点是其中的关键。
- 如果从防止模型坍塌的不同方法角度，我们可大致把现有方法划分为：基于负例的对比学习方法、基于对比聚类的方法、基于不对称网络结构的方法，以及基于冗余消除损失函数的方法。
- 在将增强图像投影到表示空间过程中，我们做了两次非线性映射，分别是Encoder 和Projector，为什么要做两次投影变换呢？
- 前文有述，对比学习在做特征表示相似性计算时，要先对表示向量做L2正则，之后再做点积计算，或者直接采用Cosine相似性，为什么要这么做呢？
- InfoNCE是个能够感知负例难度的损失函数，而之所以能做到这点，主要依赖超参。
- L2正则具体做法：
	- ![[Pasted image 20230621145012.png]]
	- 将向量每个元素除以它的L2范数
	- 结果是处理后的向量模长为1
	- 相比带有向量长度信息的点积，在去掉长度信息后的单位长度向量上操作，能增加深度学习模型的训练稳定性