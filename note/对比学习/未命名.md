## CD-TOM
- 使用对比学习，构造一个表征学习系统，将微博正文和话题映射到同一个语义空间中，在表示空间内，语义相近的正文和话题距离较为接近，而语义不相似的正文和话题距离较远。以此来获得一个针对微博内容的编码器和话题编码器，希望这两个编码器能够给微博内容和话题打出高质量的Embedding。我们称这个利用<文档，话题>数据的对比学习模型为CD-TOM模型（Contrastive Document-TOpic Model），它其实是SimCLR的自然语言处理版本。
- ![[Pasted image 20230616153157.png]]
- 目前很多研究表明，在做文本表示时，用Bert的[CLS]来表征文本内容效果不佳，所以常常会用Transformer最高层单词对应的Embedding编码（注：如果同时用第一层和最高层单词Embedding效果更好），句中每个单词对应Embedding累加后求均值（TAP, Token Average Pooling），形成博文表示向量
- 目前对比学习模型研究表明：将原始数据映射到单位超球面，一般这样做效果更好。所以接下来，我们对博文表示向量 z 做L2正则，将其映射到单位超球面上
- 我们对博文表示向量和话题表示向量进行点积，以此作为度量函数，来衡量单位超球面上任意两点的距离：
	- ![[Pasted image 20230619185250.png]]

- 而损失函数则采用对比学习常用的InfoNCE，某个例子对应的InfoNCE损失为：
	- ![[Pasted image 20230619185259.png]]
	- 
- 我们经过实验证明，放大Batch Size对于模型效果有较为明显的正向效果，通过LAMB优化器，并结合Gradient Checkpointing优化技巧，放大Batch Size到1664