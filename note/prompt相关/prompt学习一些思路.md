
1. 序列标注任务也可以转为 p-tuning的形式，<mark style="background: #FFB8EBA6;">UIE不就是这么做的吗，只不过用的是显式的prompt，但是拼凑的太生硬，我觉得加上隐式的token应该会更好</mark>
2. 也就是说大部分下游任务都可以转为p-tuning的形式去跟lm预训练对齐，才是更合理的自然的
3. 可以看到ml和clm的底层没有区别，如果gpt可以很强的话，bert没有理由在nlu任务上会比他差，bert虽然在很多下游任务上很强，但是不能做生成的话，即使理解能力再强，也不能完全表达出来，而GPT是生成形式，所以在下游任务中都是转为生成形式，生成的形式，让他更通用，自然应用也更广泛。如果能用mlm做理解，clm做生成会不会更有更好，也就是说为什么encoder-decoder相关的模型没有大放异彩，是不是潜力没挖掘出来
4. 人是怎么思考的，人在回答一个问题的时候，除了考虑上文，还会回忆相关的最近的记忆和经验，并且人的大脑神经元是时刻在更新的
