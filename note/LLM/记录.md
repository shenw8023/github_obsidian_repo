- claude https://zhuanlan.zhihu.com/p/629225773 #TODO 
- OPT
	- Open Pre-trained Transformer Language Models
- Bloom
- 打标任务，我们的llama对候选标签的长度非常敏感
	- 怎么发现的：
		- 验证原来的测试数据还是能得到正确结果，说明模型本身没变
		- 测试多个样例，发现在新数据上，输出的无关标签”计算机视觉“
		- 我把5个这些无关标签添加到原来的测试数据的候选标签后面，测试原来数据后发现，模型会把这5个无关标签也输出
		- 我用这5个无关标签替换掉原来测试数据的候选标签的5项，测试后发现就不输出这5个无关标签了，说明是候选标签太多了，导致模型不会回答了，倾向于全部输出
		- 进一步实验确定模型对候选标签的数量很敏感
		- 可能原因：
			- sft阶段的候选标签数量比较固定，导致过拟合了，自然就对标签数量敏感


- 数据多样性和丰富性的策略
	- 去重 #TODO
	- 收集数据的主题类型，任务类型
		- 参考Apalca和Chinese-LLaMA-Alpaca中的crwal_prompt.py
		- topic_list = ["科技", "娱乐", "体育", "金融", "时政", "教育"...]
		- task_list = ["开放式生成", "分类", "问答", "编辑", "摘要", "写作", "翻译"]
	 - 每个类型随机抽几十条数据，大约一千条，使用p-tuning微调一个分类模型，每个类型下收集一批数据
	 - 然后每个类型下筛选高质量的数据
	 - 最后保证整体的数据均衡，再加入一些不属于任意类型的数据的采样，提高丰富度
 - 数据集
	 - 多轮对话
		 - lianjiaTech/BELLE
	 - 思维链数据
	 - GPT生成的
	 - 真实用户提交的：shareGPT
	 - 



- 业务数据准备
	- 使用GPT对一个文段做提问，生成多种问题 #TODO 
	- 复习一下Alpaca


- 上下文学习能力的降低更多是指令学习的一个副作用，OpenAI 管这叫对齐税。
- 大模型训练围绕alighment，对齐人类喜好
	- GPT3虽然在很多nlp任务上具备很好的零样本学习能力，但是
- 指令微调的目的就是


- MOE
	- 如何进行增量训练，同时避免灾难性遗忘。方法：引入新的数据分布时，引入新的 experts，同时冻结原有的权重，并且加入一个regularization loss 避免灾难性遗忘[[7]](https://zhuanlan.zhihu.com/p/636861410#ref_7)。
- Chinchilla
	- 模型大小和训练tokens数量应该相等地scaling：模型大小加倍，token数量也应该同样翻倍
	- Chinchilla 表明我们在训练期间需要使用比 GPT-3 和类似模型多 11 倍的数据

- interv_conclude
	- 不涉及的内容不能直接说没做过，可以说了解，具体问题回答不上来的时候再说没做过
	- 对一些负责人，太多专业词汇不行，要把概念说的通俗一点
	- 