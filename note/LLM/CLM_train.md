- 多轮对话在被训练的时候，单轮也同时被训练了
- AutoModelForCausalLM 对应的是相应的model with LM head，例如GPT2LMHeadModel，XGLMForCausalLM
- 普通的clm训练

- 关于loss
	- nn.CrossEntropyLoss
	- 对每个token计算cross_entropy
	- logits:  [B, S, H] -> [B, S, n_vocab], 其中每个元素是未归一化的logit
	- labels:   [B,S] ,其中每个元素是真实token的index
	- 计算单标签多分类情况下的cross_entropy

- 关于评价指标 困惑度 perplexity，[参考](https://blog.csdn.net/qq_27586341/article/details/110952649)
	- ![[Pasted image 20230518141217.png]]
	- 用来刻画模型模型预测一个句子的概率的好坏，困惑度越高，模型越好，通常使用其对数形式
	- 实际是计算每一个单词得到的概率倒数的几何平均，因此perplexity可以理解为平均分支系数，即**模型预测下一个词时的平均可选择数量**。

	- ![[Pasted image 20230518141159.png]]
	- <mark style="background: #FF5582A6;">其对数形式就是一个序列的交叉熵</mark>，推导起来很简单
	- log perplexity可以看成真实分布与预测分布之间的**交叉熵**，即真实分布与预测分布之间的距离。
	- 区别在于，<mark style="background: #FF5582A6;">由于语言的真实分布是未知的，真实分布用测试语料的取样代替，</mark>即认为在给定上文的条件下，语料中出现单词$w_i$的概率为1，出现其他单词的概率为0。
	- 代码参考 clm_train_relate

## 关于BLEU #TODO 



## 关于batch_size的选择
- https://www.zhihu.com/question/32673260 #TODO 
- 在训练语言模型时,选择大批量还是小批量需要综合考虑多方面因素:
- 优点:
	小批量:
	- 收敛更快
	- 梯度变化大,有助于跳出局部最优
	- 对噪声敏感
	
	大批量:
	- 更稳定
	- 对噪声更鲁棒
	- 更平滑的梯度

- 缺点:
	小批量:
	- 训练过程不稳定
	- 收敛后可能效果不如大批量
	
	大批量:
	- 需要更多的步骤才能收敛
	- 收敛速度较慢

- 综上所述,在 inicio 阶段,可以使用相对较小的批量(如 64 或 128),有助于快速找到一个不错的效果。

- 在后续的训练过程中,可以逐渐增加批量规模(如 512、1024或更大),有利于平滑参数更新和获得更稳定的效果。

- 并且大批量也可以减少在线存储器需求,这在处理大模型时十分重要。

- 因此,一般都是从小批量开始,然后逐渐增加批量规模。同时也需要监控效果和收敛速度,合理选择最佳批量。

- 使用动态批量策略也可以很好地弥补小批量和大批量的缺点,这可能是一种折中而有效的选择。

