- 感觉看trainer的设计基本上能把torch的许多优化方法和训练技巧都理解一遍了。
- torch 分布式
	- DP
	- DDP
	- Accelerate
	- 
- Below is an example yaml for <mark style="background: #FFB86CA6;">BF16 </mark>mixed-precision training using PyTorch Fully Sharded Data Parallism <mark style="background: #FF5582A6;">(FSDP) </mark>with CPU offloading on 8 GPUs.

- 99.100上机器GPU索引实际为4,5,6,7,0,1,2,3