
## 资源
- 博客：
	- [Advantage Actor Critic (A2C)](https://huggingface.co/blog/deep-rl-a2c)
	- [huggingface  Deep RL Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction)
	- [李宏毅深度强化学习笔记](https://blog.csdn.net/cindy_1102/article/details/87907470)
	- 


## 大语言模型的RLHF训练
- PPO的代码实现：
	- [TRL: Transformer Reinforcement Learning](https://github.com/lvwerra/trl/tree/main)
	- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training)
		- [知乎博客代码讲解](https://zhuanlan.zhihu.com/p/624589622)
	- [trlx: Transformer Reinforcement Learning X](https://github.com/CarperAI/trlx)
		- 分布式训练框架，可以提供reward function 或者 reward-labeled dataset.
- 项目实践
	- transformer_task/RLHF
		- [github](https://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF)
		- [知乎博客](https://zhuanlan.zhihu.com/p/606328992)
		- 核心使用早期的TRL实现，不含分布式相关的工程代码，适合学习PPO实现，已debug基本理解整个训练过程，梳理笔记链接：[[trl梳理]] 
		- 该项目内容很多值得学习，包括reward model的训练和<mark style="background: #FFB8EBA6;">标注工具的实现</mark>都有



## todos
- [x] 梳理transformer_task/RLHF项目的代码
- [ ] 梳理DeepSpeed-Chat的代码
	- [ ] 排序结果怎么处理, pair wise loss
	- [ ] 分布式完整训练跑通
- [ ] 李宏毅+磨菇书 要点总结
- [ ] huggingface 的课程
- [ ] Instruct-gpt中的ppo-ptx损失有什么区别