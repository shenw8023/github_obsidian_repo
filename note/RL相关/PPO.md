
## PPO算法中，用于强化学习的样本序列可以是人工构建的高质量数据吗
PPO算法中用于强化学习的样本序列不一定非要是人工构建的高质量数据。具体来说有以下几点:
1. 对于强化学习来说,最好的样本数据是从环境中通过探索收集的。因为环境是复杂动态的,人工构建的数据难以完全模拟。
2. 高质量的数据集可能包含人设定的一些优先关系,这会影响模型自主学习真实环境中的优先关系。
3. 人工数据往往存在偏差,比如针对特定情况进行优化,可能会限制模型的泛化能力。
4. 在实际应用中,往往无法获取足够多的人工高质量数据,因此仍需要依赖模型通过与环境交互来收集数据。
5. PPO算法通过采样当前模型生成的数据并与环境交互来更新模型,强化学习的效果主要依赖这个采样-更新循环。人工数据只适合作为辅助学习的初期训练数据。

所以总的来说,PPO算法更倾向于通过自主与环境交互来收集样本,人工数据集可以作为起步时候的初步训练,但不能完全替代通过环境交互获得的数据。最佳效果需要将两者结合使用。



## RLHF算法用于训练大语言模型时，标准强化学习中的”环境“对应这里的什么
对于训练大语言模型这种任务来说,标准强化学习中的"环境"对应这里的语言模型本身。
具体来说:
- 状态(State):当前语言模型已看到的文本序列。
- 动作(Action): 选择下一个要生成的单词。
- 转移函数(Transition Function): 描述如何从当前状态和动作转移到下一个状态。对于语言模型来说,就是将选择的下一个单词加入到文本序列中,形成下一个状态。
- 回报函数(Reward Function):评估生成的单词与接下来真实出现的单词的一致度,给出相应的回报信号。

- 总的来说，环境的作用是根据一个action，得到下一个state，这里的环境发挥的作用就是将预测的word拼接到上文后面，作为新的上文，作为状态，用于产生下一步action




- 为什么variance会大
	- But the problem is that **the variance is high, since trajectories can lead to different returns** due to stochasticity of the environment (random events during episode) and stochasticity of the policy.

![[Pasted image 20230606175425.png]]